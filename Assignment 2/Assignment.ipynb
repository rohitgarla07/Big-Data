{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Name: Rohit Garla</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Net ID:rg3365</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Name: Mengrui Zhang</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Net ID:mz2258</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>Assignment 2</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/Users/garlarohit/desktop/Assignment 1/mortgage_sample;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o28.parquet.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/garlarohit/desktop/Assignment 1/mortgage_sample;\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:626)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:349)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:559)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2e817c56e656>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/Users/garlarohit/desktop/Assignment 1/mortgage_sample\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select count(FICO) from data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, *paths)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'string'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Path does not exist: file:/Users/garlarohit/desktop/Assignment 1/mortgage_sample;'"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/Users/garlarohit/desktop/Assignment 1/mortgage_sample\")\n",
    "df.show()\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql(\"select count(FICO) from data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql import Row\n",
    "import math\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|  92996|\n",
      "|    3|  27095|\n",
      "|    2|2602038|\n",
      "+-----+-------+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  350|\n",
      "|    2| 9554|\n",
      "|    3|   96|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 1532|\n",
      "|    2| 8386|\n",
      "|    3|   82|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 1666|\n",
      "|    2| 8242|\n",
      "|    3|   92|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 1864|\n",
      "|    2| 8033|\n",
      "|    3|  103|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+--------+--------------------+----+--------------+--------+----------+------+-----+--------------------+--------------------+--------------------+----------+\n",
      "| FICO|CLEANLTV|           INCENTIVE| AGE|FIRSTTIMEBUYER|INVESTOR|SECONDHOME|SEASON|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------+--------------------+----+--------------+--------+----------+------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|751.0|    61.0|0.006200000000000004|19.0|           0.0|     0.0|       0.0|  12.0|    1|[751.0,61.0,0.006...|[0.0,4.6140185263...|[0.0,0.3845015438...|       2.0|\n",
      "|751.0|    61.0|0.008200000000000006|20.0|           0.0|     0.0|       0.0|   1.0|    1|[751.0,61.0,0.008...|[0.0,3.8659566512...|[0.0,0.3221630542...|       2.0|\n",
      "|751.0|    61.0|0.007699999999999...|29.0|           0.0|     0.0|       0.0|  10.0|    1|[751.0,61.0,0.007...|[0.0,3.5712280421...|[0.0,0.2976023368...|       2.0|\n",
      "|751.0|    61.0|0.008099999999999996|30.0|           0.0|     0.0|       0.0|  11.0|    1|[751.0,61.0,0.008...|[0.0,3.3740851849...|[0.0,0.2811737654...|       2.0|\n",
      "|751.0|    61.0|0.012300000000000005|35.0|           0.0|     0.0|       0.0|   4.0|    2|[751.0,61.0,0.012...|[0.0,2.2592256792...|[0.0,0.1882688066...|       2.0|\n",
      "|751.0|    61.0|0.004299999999999998|37.0|           0.0|     0.0|       0.0|   6.0|    2|[751.0,61.0,0.004...|[0.0,0.5135017815...|[0.0,0.0427918151...|       2.0|\n",
      "|798.0|    53.0|0.003600000000000006| 0.0|           0.0|     0.0|       0.0|  11.0|    2|(8,[0,1,2,7],[798...|[0.0,0.1563237726...|[0.0,0.0130269810...|       2.0|\n",
      "|798.0|    53.0|0.001200000000000...| 6.0|           0.0|     0.0|       0.0|   5.0|    2|[798.0,53.0,0.001...|[0.0,0.4460001508...|[0.0,0.0371666792...|       2.0|\n",
      "|798.0|    53.0|0.004500000000000004| 8.0|           0.0|     0.0|       0.0|   7.0|    2|[798.0,53.0,0.004...|[0.0,1.9739920209...|[0.0,0.1644993350...|       2.0|\n",
      "|798.0|    53.0|0.007800000000000008|10.0|           0.0|     0.0|       0.0|   9.0|    2|[798.0,53.0,0.007...|[0.0,3.7181866090...|[0.0,0.3098488840...|       2.0|\n",
      "|793.0|    41.0|0.002149999999999999| 1.0|           0.0|     0.0|       0.0|   4.0|    2|[793.0,41.0,0.002...|[0.0,0.9562909302...|[0.0,0.0796909108...|       2.0|\n",
      "|793.0|    41.0|0.004050000000000012| 7.0|           0.0|     0.0|       0.0|  10.0|    1|[793.0,41.0,0.004...|[0.0,3.3203345324...|[0.0,0.2766945443...|       2.0|\n",
      "|793.0|    41.0|0.001050000000000...| 9.0|           0.0|     0.0|       0.0|  12.0|    2|[793.0,41.0,0.001...|[0.0,0.2445812807...|[0.0,0.0203817733...|       2.0|\n",
      "|793.0|    41.0| 0.00785000000000001|17.0|           0.0|     0.0|       0.0|   8.0|    1|[793.0,41.0,0.007...|[0.0,4.3099490519...|[0.0,0.3591624209...|       2.0|\n",
      "|793.0|    41.0|0.011350000000000006|19.0|           0.0|     0.0|       0.0|  10.0|    2|[793.0,41.0,0.011...|[0.0,4.0433967653...|[0.0,0.3369497304...|       2.0|\n",
      "|788.0|    80.0|-2.99999999999994...| 5.0|           0.0|     0.0|       0.0|   8.0|    2|[788.0,80.0,-2.99...|[0.0,0.1858101344...|[0.0,0.0154841778...|       2.0|\n",
      "|788.0|    80.0|0.002000000000000...|12.0|           0.0|     0.0|       0.0|   3.0|    2|[788.0,80.0,0.002...|[0.0,1.5641593649...|[0.0,0.1303466137...|       2.0|\n",
      "|788.0|    80.0|0.004500000000000004|16.0|           0.0|     0.0|       0.0|   7.0|    2|[788.0,80.0,0.004...|[0.0,0.9586318691...|[0.0,0.0798859890...|       2.0|\n",
      "|788.0|    80.0|0.010900000000000007|25.0|           0.0|     0.0|       0.0|   4.0|    2|[788.0,80.0,0.010...|[0.0,3.9033191252...|[0.0,0.3252765937...|       2.0|\n",
      "|788.0|    80.0| 0.00860000000000001|29.0|           0.0|     0.0|       0.0|   8.0|    2|[788.0,80.0,0.008...|[0.0,2.7601679162...|[0.0,0.2300139930...|       2.0|\n",
      "+-----+--------+--------------------+----+--------------+--------+----------+------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.limit(10000)\n",
    "df.groupBy('label').count().show()\n",
    "df2.groupBy('label').count().show()\n",
    "fractions = {1: 1, 2: 0.2, 3: 0.2}\n",
    "df3 = df.stat.sampleBy(\"label\", fractions, 12).limit(10000)\n",
    "df3.groupBy('label').count().show()\n",
    "fractions = {1: 1, 2: 0.18, 3: 0.2}\n",
    "df4 = df.stat.sampleBy(\"label\", fractions, 12).limit(10000)\n",
    "df4.groupBy('label').count().show()\n",
    "fractions = {1: 1, 2: 0.16, 3: 0.2}\n",
    "df5 = df.stat.sampleBy(\"label\", fractions, 12).limit(10000)\n",
    "df5.groupBy('label').count().show()\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=12,  maxDepth=10)\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "model = rf.fit(df3)\n",
    "model1 = rf.fit(df4)\n",
    "lrModel = lr.fit(df5)\n",
    "pred = model.transform(df3)\n",
    "pred1 = model1.transform(df4)\n",
    "pred2 = lrModel.transform(df5)\n",
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial coefficients: DenseMatrix([[ -8.96215336e-04,  -1.56239975e-03,  -8.87493146e-01,\n",
      "               -3.75400567e-04,  -5.20023376e-03,  -4.80593573e-03,\n",
      "               -4.78263121e-03,  -2.61713366e-03],\n",
      "             [  1.72915021e-03,  -1.32352183e-02,   3.76449456e+01,\n",
      "               -9.17912597e-03,  -6.06413175e-02,  -3.99191287e-01,\n",
      "               -1.64370725e-01,   1.59645654e-02],\n",
      "             [  1.89525143e-03,  -7.31940466e-03,  -6.89399735e+01,\n",
      "                1.88828396e-05,   2.43537855e-02,   2.76796727e-01,\n",
      "                6.94222871e-02,   3.86378857e-03],\n",
      "             [ -2.72818630e-03,   2.21170227e-02,   3.21825211e+01,\n",
      "                9.53564370e-03,   4.14877658e-02,   1.27200496e-01,\n",
      "                9.97310687e-02,  -1.72112203e-02]])\n",
      "Multinomial intercepts: [-5.29636850861,2.13578062824,3.78207362107,-0.621485740706]\n"
     ]
    }
   ],
   "source": [
    "print(\"Multinomial coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7189677764903939"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.evaluate(pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.23577590805946735\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "accuracy = evaluator.evaluate(pred1)\n",
    "print(\"Test Error = \" + str(1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.22048781520941518\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "print(\"Test Error = \" + str(1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  _1|\n",
      "+--------------------+\n",
      "| 0.49171341241220595|\n",
      "|  0.3914155544992621|\n",
      "| 0.35872024809906233|\n",
      "| 0.33314678839709644|\n",
      "|  0.2126039635323621|\n",
      "| 0.04373437213380192|\n",
      "|0.014663011479900548|\n",
      "| 0.03899098057675509|\n",
      "| 0.18011535121556285|\n",
      "|  0.3718111356701752|\n",
      "|  0.0907523236955305|\n",
      "| 0.32437557559416624|\n",
      "| 0.03288051902358533|\n",
      "| 0.44533363122557135|\n",
      "| 0.41225143178829127|\n",
      "|0.016368581717375853|\n",
      "| 0.14107539264398364|\n",
      "| 0.08325769172184137|\n",
      "| 0.39576793123603715|\n",
      "|  0.2644348922827095|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.rdd.map(lambda x : -math.log(x[\"probability\"][int(x[\"prediction\"])])).map(lambda x : Row(x)).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pf = pred.rdd.map(lambda x : -math.log(x[\"probability\"][int(x[\"prediction\"])])).map(lambda x : Row(x)).toDF()\n",
    "pf1 = pred1.rdd.map(lambda x : -math.log(x[\"probability\"][int(x[\"prediction\"])])).map(lambda x : Row(x)).toDF()\n",
    "pf2=pred2.rdd.map(lambda x : -math.log(x[\"probability\"][int(x[\"prediction\"])])).map(lambda x : Row(x)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: double (nullable = true)\n",
      "\n",
      "+-------------------+\n",
      "|            avg(_1)|\n",
      "+-------------------+\n",
      "|0.18846371663706205|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|            avg(_1)|\n",
      "+-------------------+\n",
      "|0.20423527885619078|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|            avg(_1)|\n",
      "+-------------------+\n",
      "|0.22423213650254825|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf.printSchema()\n",
    "pf.createOrReplaceTempView(\"entropy\")\n",
    "spark.sql(\"select avg(_1) from entropy\").show()\n",
    "pf1.createOrReplaceTempView(\"entropy\")\n",
    "spark.sql(\"select avg(_1) from entropy\").show()\n",
    "pf2.createOrReplaceTempView(\"entropy\")\n",
    "spark.sql(\"select avg(_1) from entropy\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
